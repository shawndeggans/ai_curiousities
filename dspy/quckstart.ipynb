{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Before we delve into the example, let's ensure our environment is properly configured. We'll start by importing the necessary modules and configuring our language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the OpenAI API key\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.datasets.gsm8k import GSM8K, gsm8k_metric\n",
    "\n",
    "# Set up the LM\n",
    "turbo = dspy.OpenAI(model='gpt-3.5-turbo-0125', max_tokens=250)\n",
    "dspy.settings.configure(lm=turbo)\n",
    "\n",
    "# Load math questions from the GSM8K dataset\n",
    "gsm8k = GSM8K()\n",
    "gsm8k_trainset, gsm8k_devset = gsm8k.train[:10], gsm8k.dev[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gsm8k_trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Module\n",
    "\n",
    "With our environment set up, let's define a custom program that utilizes the ChainOfThought module to perform step-by-step reasoning to generate answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(\"question -> answer\")\n",
    "    \n",
    "    def forward(self, question):\n",
    "        return self.prog(question=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and Evaluate the Model\n",
    "\n",
    "With our simple program in place, let's move on to optimizing it using the BootstrapFewShot teleprompter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "# Set up the optimizer: we want to \"bootstrap\" (i.e., self-generate) 4-shot examples of our CoT program.\n",
    "config = dict(max_bootstrapped_demos=4, max_labeled_demos=4)\n",
    "\n",
    "# Optimize! Use the `gsm8k_metric` here. In general, the metric is going to tell the optimizer how well it's doing.\n",
    "teleprompter = BootstrapFewShot(metric=gsm8k_metric, **config)\n",
    "optimized_cot = teleprompter.compile(CoT(), trainset=gsm8k_trainset, valset=gsm8k_devset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shawn's notes\n",
    "So I have issues with this starter module. This is a good example of how not to create a quickstart. OpenAI, even with the pro subscription, throttles the hell out of API requests, even on the older models like GPT 3X. It's to the point to being useless.\n",
    "\n",
    "So I'm just going to assume that this works as advertized and hope I can find a solution as I continue to learn DSPy. Connecting to my own local model would be a much better solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
